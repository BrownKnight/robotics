{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Following\n",
    "This notebook uses the COCO pretrained model to detect the location of people in front of it, then move towards the cloest person - as determined by the robots depth camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained object detection model\n",
    "When the model is used later on, we will only look at the \"person\" category even though the model will output bounding boxes for all sorts of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes, TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import numpy as np\n",
    "\n",
    "from Camera import Camera\n",
    "\n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "\n",
    "class ObjectDetector(object):\n",
    "\n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],\n",
    "                                  output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "\n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "\n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# use traitlets and widgets to display the image in Jupyter Notebook\n",
    "\n",
    "# use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# using realsense to capture the color and depth image\n",
    "\n",
    "# multi-threading is used to capture the image in real time performance\n",
    "\n",
    "\n",
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "\n",
    "# create a camera object\n",
    "# camera = Camera.instance()\n",
    "# camera.start()  # start capturing the data\n",
    "camera = Camera.instance()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the camera inpout, and move towards people\n",
    "Human is labeled is 1 in the pretrained model. A full list of the detection class indices can be found in the following \n",
    "link https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt\n",
    "\n",
    "The program will look at all the people found in the image, and determine using the cameras depth sensor where the \n",
    "location of the closest person is. It will then move towards the person using the centre x coordinate to determine how \n",
    "far left/right to move. It alos uses the distance to determine how fast it should move towards/away from the person. \n",
    "It will stop within a given distance in front of the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "display(image_widget)\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "# initialize the Robot class\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def normalize_distance(value):\n",
    "    bounds = {'actual': {'lower': 0, 'upper': 4000}, 'desired': {'lower': 60, 'upper': 460}}\n",
    "    return int(bounds['desired']['lower'] + (value - bounds['actual']['lower']) * (\n",
    "                bounds['desired']['upper'] - bounds['desired']['lower']) / (\n",
    "                           bounds['actual']['upper'] - bounds['actual']['lower']))\n",
    "\n",
    "\n",
    "def processing(change):\n",
    "    image = change['new']\n",
    "\n",
    "    imgsized = cv2.resize(image, (300, 300))\n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    # DEBUG\n",
    "    # detections = [[{'label':None}]]\n",
    "\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == 1]\n",
    "\n",
    "    # Draw some lines on the image to show the boundary lines\n",
    "    fast_left = 220\n",
    "    slow_left = 270\n",
    "    slow_right = 380\n",
    "    fast_right = 420\n",
    "\n",
    "    # Values are the bottom of the range i.e. will move fast backwards if less than distance_really_close\n",
    "    distance_really_close = 1800\n",
    "    distance_close = 2200\n",
    "    distance_stable = 2600\n",
    "    distance_far = 3000\n",
    "    distance_really_far = 3600\n",
    "\n",
    "    draw_guidance_lines(fast_left, slow_left, fast_right, slow_right, distance_really_close, distance_close,\n",
    "                        distance_stable, distance_far, distance_really_far, image)\n",
    "\n",
    "    closest_person = (0, 9999, 9999)\n",
    "    for index, det in enumerate(matching_detections):\n",
    "        bbox = det['bbox']\n",
    "        x = int(width * bbox[0])\n",
    "        y = int(height * bbox[1])\n",
    "        x1 = int(width * bbox[2])\n",
    "        y1 = int(height * bbox[3])\n",
    "        half_height = int((y1 - y) / 2)\n",
    "        quarter_width = int((x1 - x) / 4)\n",
    "        centre = (x + x1) / 2\n",
    "        # Draw a blue rectangle around every person it detects\n",
    "        cv2.rectangle(image, (x, y), (x1, y1), (255, 0, 0), 2)\n",
    "        # Draw a thin blue rectangle around the area of the bounding box we actually use to determine the distance\n",
    "        cv2.rectangle(image, (x + quarter_width, y), (x1 - quarter_width, y1 - half_height), (150, 10, 10), 2)\n",
    "        distance = np.average(camera.depth_image[y:y1 - half_height, x + quarter_width:x1 - quarter_width])\n",
    "        cv2.putText(image, \"D %.0f\" % distance, (x + 10, y + half_height), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255),\n",
    "                    2)\n",
    "        closest_person = ((x, y, x1, y1), centre, distance) if distance < closest_person[2] else closest_person\n",
    "\n",
    "    speed = 0\n",
    "    turn_speed = 0.2\n",
    "    # Determine how to move\n",
    "    if len(matching_detections) != 0:\n",
    "        x, y, x1, y1 = closest_person[0]\n",
    "        distance = closest_person[2]\n",
    "        cv2.circle(image, (40, normalize_distance(distance)), 4, (0, 165, 255), -1)\n",
    "        cv2.rectangle(image, (x, y), (x1, y1), (255, 255, 255), 3)\n",
    "        centre = closest_person[1]\n",
    "        cv2.circle(image, (int(centre), 40), 4, (0, 165, 255), -1)\n",
    "\n",
    "        if centre < fast_left:\n",
    "            turn_speed = -0.7\n",
    "        elif centre < slow_left:\n",
    "            turn_speed = -0.4\n",
    "        elif slow_left < centre < slow_right:\n",
    "            turn_speed = 0.0\n",
    "        elif slow_right < centre < fast_right:\n",
    "            turn_speed = 0.4\n",
    "        elif centre >= fast_right:\n",
    "            turn_speed = 0.7\n",
    "\n",
    "        if distance < distance_really_close:\n",
    "            speed = -0.5\n",
    "        elif distance < distance_close:\n",
    "            speed = -0.2\n",
    "        elif distance < distance_stable:\n",
    "            speed = 0\n",
    "        elif distance < distance_far:\n",
    "            speed = 0.2\n",
    "        elif distance < distance_really_far:\n",
    "            speed = 0.5\n",
    "        else:\n",
    "            speed = 0.6\n",
    "\n",
    "    left_speed = speed + turn_speed\n",
    "    right_speed = speed - turn_speed\n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)\n",
    "\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "\n",
    "def draw_guidance_lines(fast_left, slow_left, fast_right, slow_right, distance_really_close, distance_close,\n",
    "                        distance_stable, distance_far, distance_really_far, image):\n",
    "    # Draw guidance lines at top of the screen for moving left/right\n",
    "    colour = (0, 165, 255)\n",
    "    add_direction_guideline(colour, fast_left, image, \"<<<<\")\n",
    "    add_direction_guideline(colour, slow_left, image, \"<<\")\n",
    "    add_direction_guideline(colour, slow_right, image, \"----\")\n",
    "    add_direction_guideline(colour, fast_right, image, \">>\")\n",
    "    cv2.putText(image, \">>>>\", (fast_right + 40, 20), cv2.FONT_HERSHEY_PLAIN, 1, colour, 1)\n",
    "\n",
    "\n",
    "    # Draw guidance on left side of screen for distance markers\n",
    "    top_boundary = 60\n",
    "    bottom_boundary = 460\n",
    "    cv2.line(image, (5, top_boundary), (55, top_boundary), colour, 2)\n",
    "    add_distance_guideline(colour, distance_really_close, image, \"----\")\n",
    "    add_distance_guideline(colour, distance_close, image, \"--\")\n",
    "    add_distance_guideline(colour, distance_stable, image, \"STOP\")\n",
    "    add_distance_guideline(colour, distance_far, image, \"++\")\n",
    "    add_distance_guideline(colour, distance_really_far, image, \"++++\")\n",
    "    cv2.line(image, (5, bottom_boundary), (55, bottom_boundary), colour, 2)\n",
    "\n",
    "\n",
    "def add_direction_guideline(colour, x_coord, image, string):\n",
    "    cv2.line(image, (x_coord, 20), (x_coord, 60), colour, 1)\n",
    "    cv2.putText(image, string, (x_coord - 16*len(string), 20), cv2.FONT_HERSHEY_PLAIN, 1, colour, 1)\n",
    "\n",
    "\n",
    "def add_distance_guideline(colour, distance, image, string):\n",
    "    normalised = normalize_distance(distance)\n",
    "    cv2.line(image, (20, normalised), (40, normalised), colour, 1)\n",
    "    cv2.putText(image, string, (45, normalised-15), cv2.FONT_HERSHEY_PLAIN, 1, colour, 1)\n",
    "\n",
    "\n",
    "# the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "processing({'new': camera.color_value})\n",
    "camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to stop the capturing of the image and the moving of the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}